services:
  postgres:
    image: postgres:13
    container_name: postgres_db
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    networks:
      - data_network

  spark-master:
    image: bitnami/spark:3
    container_name: spark_master
    environment:
      - SPARK_MODE=master
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    networks:
      - data_network

  spark-worker:
    image: bitnami/spark:3
    container_name: spark_worker
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    networks:
      - data_network

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_init
    depends_on:
      - postgres
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__WEBSERVER__SECRET_KEY=GUdcCwzi8sz1hZ1cilD7RaNzfUxTP-FCNx1lSsPaG-c=
    command: >
      bash -c "
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin
      "
    networks:
      - data_network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_webserver
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports:
      - "8080:8080"
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW__WEBSERVER__SECRET_KEY=GUdcCwzi8sz1hZ1cilD7RaNzfUxTP-FCNx1lSsPaG-c=
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    networks:
      - data_network
    command: airflow webserver

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    container_name: airflow_scheduler
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark-master:7077
      - AIRFLOW__WEBSERVER__SECRET_KEY=GUdcCwzi8sz1hZ1cilD7RaNzfUxTP-FCNx1lSsPaG-c=
    volumes:
      - ./dags:/opt/airflow/dags
      - ./spark_jobs:/opt/bitnami/spark/jobs
      - ./data:/opt/bitnami/spark/data
    networks:
      - data_network
    command: airflow scheduler

  metabase:
    image: metabase/metabase:latest
    container_name: metabase_dashboard
    ports:
      - "3000:3000"
    networks:
      - data_network
      
volumes:
  postgres_data:

networks:
  data_network:
    driver: bridge